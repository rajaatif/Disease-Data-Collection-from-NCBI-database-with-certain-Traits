{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eDS9pZh_81-H",
        "BbxY8JOuyMOM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfpSO5tEY3ab",
        "outputId": "dea08cac-1f8a-4846-f183-1720585bae6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/3.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Any Fasta Refrence Genome you want to Align"
      ],
      "metadata": {
        "id": "vM0BHCMSFXZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install ncbi-blast+\n",
        "!makeblastdb -in sequence.fasta -dbtype nucl -out local_hbv_reference_db\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo00NnetKOuk",
        "outputId": "cd61862c-b4d5-4846-aaa8-424e80a2ff2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ncbi-blast+ is already the newest version (2.12.0+ds-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "\n",
            "\n",
            "Building a new DB, current time: 11/07/2024 20:44:19\n",
            "New DB name:   /content/local_hbv_reference_db\n",
            "New DB title:  sequence.fasta\n",
            "Sequence type: Nucleotide\n",
            "Deleted existing Nucleotide BLAST database named /content/local_hbv_reference_db\n",
            "Keep MBits: T\n",
            "Maximum file size: 1000000000B\n",
            "Adding sequences from FASTA; added 1 sequences in 0.000357151 seconds.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helping Function"
      ],
      "metadata": {
        "id": "JR-FX7pa8yRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_regions(region_dicts, total_length):\n",
        "  regions = []\n",
        "  for region_dict in region_dicts:\n",
        "      start = region_dict['query_start']\n",
        "      end = region_dict['query_end']\n",
        "      if 0 <= start < total_length and 0 < end <= total_length:\n",
        "          regions.append((start, end))\n",
        "  return regions\n",
        "def find_uncovered_regions(region_dicts, total_length):\n",
        "    \"\"\"\n",
        "    Finds regions that are not covered by the given region dictionaries.\n",
        "\n",
        "    Args:\n",
        "        region_dicts: A list of dictionaries, where each dictionary represents a region\n",
        "                      with 'query_start' and 'query_end' keys.\n",
        "        total_length: The total length of the sequence.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple represents an uncovered region (start, end).\n",
        "    \"\"\"\n",
        "    covered_regions = extract_regions(region_dicts, total_length)  # Get the covered regions\n",
        "    covered_regions.sort()  # Sort the regions by start position\n",
        "\n",
        "    uncovered_regions = []\n",
        "    last_end = 0  # Initialize the end of the last covered region to 0\n",
        "\n",
        "    for start, end in covered_regions:\n",
        "        if start > last_end:  # If there is a gap between the last covered region and the current one\n",
        "            uncovered_regions.append((last_end, start))  # Add the gap as an uncovered region\n",
        "        last_end = max(last_end, end)  # Update the end of the last covered region\n",
        "\n",
        "    if last_end < total_length:  # If there is an uncovered region at the end\n",
        "        uncovered_regions.append((last_end, total_length))  # Add it to the list\n",
        "\n",
        "    return uncovered_regions\n",
        "def hamming_distance(seq1, seq2):\n",
        "    \"\"\"Calculates the Hamming distance between two sequences.\"\"\"\n",
        "    if len(seq1) != len(seq2):\n",
        "        raise ValueError(\"Sequences must have the same length\")\n",
        "    distance = sum(c1 != c2 for c1, c2 in zip(seq1, seq2))\n",
        "    return distance"
      ],
      "metadata": {
        "id": "6aCH1qkuTH9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Blast.Applications import NcbiblastnCommandline\n",
        "from Bio.Blast import NCBIXML\n",
        "\n",
        "\n",
        "def complete_genome(hbv_genome_sequence,ref):\n",
        "\n",
        "    #print(f\"Length of query sequence: {len(hbv_genome_sequence)}\")\n",
        "\n",
        "    # Step 1: Save your sequence to a temporary file in FASTA format\n",
        "    with open(\"hbv_genome_query.fasta\", \"w\") as f:\n",
        "        f.write(\">HBV_genome\\n\")\n",
        "        f.write(hbv_genome_sequence)\n",
        "\n",
        "    # Step 2: Path to your local BLAST database (the reference HBV genome database)\n",
        "    db_path = \"local_hbv_reference_db\"  # This is the BLAST database created from the reference genome\n",
        "\n",
        "    # Step 3: Perform BLAST search for exact match (100% identity)\n",
        "    blastn_cline = NcbiblastnCommandline(query=\"hbv_genome_query.fasta\", db=db_path, evalue=0.001, outfmt=5, out=\"blast_output.xml\",perc_identity=50)\n",
        "\n",
        "    # Run the BLAST search and capture stdout/stderr\n",
        "    stdout, stderr = blastn_cline()\n",
        "    total_aligned=[]\n",
        "    # Step 4: Parse the BLAST results to extract gene regions based on the alignment\n",
        "    reference_genome = \"\"  # This will hold your entire reference genome sequence\n",
        "\n",
        "    # Load the reference genome from your BLAST database (make sure you have access to it)\n",
        "    with open(ref+\".fasta\", \"r\") as ref_file:\n",
        "        reference_genome = \"\".join(ref_file.readlines()[1:]).strip()  # Get the sequence part (skip header)\n",
        "    reference_genome=reference_genome.replace(\"\\n\",\"\")\n",
        "\n",
        "    # Parse the BLAST results and modify the reference genome\n",
        "    with open(\"blast_output.xml\") as result_handle:\n",
        "        blast_records = NCBIXML.parse(result_handle)\n",
        "        for record in blast_records:\n",
        "\n",
        "            for alignment in record.alignments:\n",
        "                for hsp in alignment.hsps:\n",
        "                    # Replace the aligned portion of the reference genome with the hbv_genome_sequence\n",
        "                    start_pos = hsp.sbjct_start - 1  # Convert to 0-based indexing\n",
        "                    end_pos = hsp.sbjct_end  # End position is inclusive, so no change needed\n",
        "                    query_start = hsp.query_start -1\n",
        "                    query_end = hsp.query_end\n",
        "                   # print(query_start,query_end,start_pos,end_pos)\n",
        "                    # Construct the modified reference genome with the aligned region replaced by the query sequence\n",
        "                    modified_genome = (\n",
        "                        reference_genome[:start_pos] +  # Part before the alignment\n",
        "                        hbv_genome_sequence[query_start:query_end] +  # Replace the aligned region with the query sequence\n",
        "                        reference_genome[end_pos:]  # Part after the alignment\n",
        "                    )\n",
        "                    total_aligned.append({'query_start':query_start,'query_end':query_end})\n",
        "    try:\n",
        "      return modified_genome,total_aligned,reference_genome\n",
        "    except:\n",
        "      return modified_genome,total_aligned,reference_genome\n",
        "\n"
      ],
      "metadata": {
        "id": "xBKfE2sL8-Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2133c2-8d13-46c4-c627-9ad96f83d5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
            "\n",
            "Due to the on going maintenance burden of keeping command line application\n",
            "wrappers up to date, we have decided to deprecate and eventually remove these\n",
            "modules.\n",
            "\n",
            "We instead now recommend building your command line and invoking it directly\n",
            "with the subprocess module.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def modified_data(input,reference_genome='sequence'):\n",
        "\n",
        "    modified_genome,total_aligned,reference_genome=complete_genome(input,reference_genome)\n",
        "    try:\n",
        "        uncovered_regions=find_uncovered_regions(total_aligned,len(input))\n",
        "        seq1 = modified_genome #[0:3182]\n",
        "        seq2 = reference_genome #[0:3182]\n",
        "        distance = hamming_distance(seq1, seq2)\n",
        "        dissimilarity_percentage = (distance / len(seq1)) * 100\n",
        "        return modified_genome,dissimilarity_percentage,uncovered_regions\n",
        "    except:\n",
        "        return modified_genome,0.00,uncovered_regions"
      ],
      "metadata": {
        "id": "krRxYF4Fp32a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download HBV Sequence from NCBI Repository using API"
      ],
      "metadata": {
        "id": "lokZYtSMEBNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "from Bio import Entrez\n",
        "import csv\n",
        "\n",
        "def download_sequences_with_metadata(query, num_sequences, output_file):\n",
        "    # Set up Entrez email address (required by NCBI)\n",
        "    Entrez.email = 'your_email@example.com'\n",
        "\n",
        "    # Perform the search on NCBI's Nucleotide database\n",
        "    handle = Entrez.esearch(db='nucleotide', term=query, retmax=num_sequences)\n",
        "    search_results = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    # Fetch the sequences along with metadata based on the search results\n",
        "    id_list = search_results['IdList']\n",
        "    handle = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb', retmode='xml')\n",
        "    records = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    # Open a CSV file to write the sequences and metadata\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write header with metadata columns\n",
        "        writer.writerow(['Sequence ID', 'Sequence', 'Isolation Source', 'Genotype', 'Collection Date', 'Organism', 'Accession Number','Gene','loc'])\n",
        "\n",
        "        # Iterate over the records to extract metadata and sequence\n",
        "        for record in records:\n",
        "            sequence_id = record.get('GBSeq_locus', 'N/A')\n",
        "            sequence = record.get('GBSeq_sequence', 'N/A')\n",
        "            isolation_source = extract_qualifier(record, 'isolation_source')\n",
        "            genotype = extract_qualifier(record, 'genotype')\n",
        "            collection_date = extract_qualifier(record, 'collection_date')\n",
        "            organism = record.get('GBSeq_organism', 'N/A')\n",
        "            accession_number = record.get('GBSeq_accession-version', 'N/A')\n",
        "            gene_name = extract_qualifier(record, 'gene')\n",
        "            location = extract_qualifier(record, 'note')\n",
        "\n",
        "            # Write the data to the CSV file\n",
        "            writer.writerow([sequence_id, sequence, isolation_source, genotype, collection_date, organism, accession_number,gene_name,location])\n",
        "\n",
        "    print(f'{len(records)} sequences downloaded and saved to {output_file}.')\n",
        "\n",
        "def extract_qualifier(record, qualifier_name):\n",
        "    \"\"\"\n",
        "    Extracts a specific qualifier from a GenBank record.\n",
        "    \"\"\"\n",
        "    for feature in record.get('GBSeq_feature-table', []):\n",
        "        for qualifier in feature.get('GBFeature_quals', []):\n",
        "            if qualifier.get('GBQualifier_name') == qualifier_name:\n",
        "                return qualifier.get('GBQualifier_value', 'N/A')\n",
        "    return 'N/A'\n",
        "\n",
        "# Example usage\n",
        "query = 'HBV AND HCC'\n",
        "num_sequences = 100000  # Adjust as needed\n",
        "output_file = 'hcc1.csv'\n",
        "\n",
        "download_sequences_with_metadata(query, num_sequences, output_file)"
      ],
      "metadata": {
        "id": "TM8xdwliUQ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "query = 'hbv carcinoma tumor'\n",
        "num_sequences = 100000  # Adjust as needed\n",
        "output_file = 'hcc2.csv'\n",
        "\n",
        "download_sequences_with_metadata(query, num_sequences, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9IK1ggxC0A-",
        "outputId": "850c7693-71ff-46f1-95f7-f5b507cf1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1010 sequences downloaded and saved to hcc2.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data from Repository"
      ],
      "metadata": {
        "id": "ot3WOR9mEYrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Google Drive public link\n",
        "url = 'https://drive.google.com/uc?id=1jFJ1Xe8ZBwxQWIhrn3hHjGhZZA1s3izT'\n",
        "\n",
        "# Define the file name to save the DataFrame\n",
        "output_file = 'final.csv'\n",
        "\n",
        "# Download the file from the Google Drive link\n",
        "gdown.download(url, output_file, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "qOxi9qdSDIN9",
        "outputId": "b135a55c-8670-4749-ee71-ece1b2ab9580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jFJ1Xe8ZBwxQWIhrn3hHjGhZZA1s3izT\n",
            "To: /content/final.csv\n",
            "100%|██████████| 75.3M/75.3M [00:01<00:00, 73.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'final.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "dataset=pd.read_csv(\"final.csv\")\n",
        "dataset= dataset.loc[dataset['Vrius_Gene'].isin(['X', 'S','C','P'])]#.sample(1000)\n",
        "\n",
        "\n",
        "\n",
        "dataset=dataset.dropna(subset=['Disease'])\n",
        "dataset=dataset.drop_duplicates(subset=['Sequence'])"
      ],
      "metadata": {
        "id": "HykvcmOWDGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hcc1 = pd.read_csv('hcc1.csv')\n",
        "\n",
        "# Load the second file into a DataFrame.\n",
        "hcc2 = pd.read_csv('hcc2.csv')\n",
        "\n",
        "# Merge the two DataFrames.\n",
        "concatenated_df = pd.concat([hcc1, hcc2], axis=0, ignore_index=True)\n",
        "organisms_to_exclude = ['Homo sapiens', 'Mus musculus', 'Rattus norvegicus','Hepacivirus hominis',\n",
        "                        'Larimichthys crocea', 'Triplophysa tibetana',\n",
        "                        'Collichthys lucidus', 'Macaca mulatta', 'Bos taurus']\n",
        "\n",
        "# Drop rows containing the specified organisms\n",
        "filtered_df = concatenated_df[~concatenated_df['Organism'].isin(organisms_to_exclude)].dropna(subset='Isolation Source')\n",
        "\n",
        "# View the filtered data\n",
        "filtered_df.head()\n",
        "organisms_to_exclude = ['non-tumor tissue', 'HBsAg positive non-HCC control','serum of HBsAg-positive patient', 'serum',\n",
        "       'serum of HBsAg-negative patient',\n",
        "       'serum of HBsAg-negative subject (control)', 'biopsy',\n",
        "       'cirrhotic patient','HBsAg negative non-HCC control','Shiraz hospitals']\n",
        "\n",
        "# Drop rows containing the specified organisms\n",
        "filtered_df = filtered_df[~filtered_df['Isolation Source'].isin(organisms_to_exclude)]\n",
        "filtered_df=filtered_df.drop_duplicates(subset='Sequence ID') #['Organism'].unique()\n",
        "filtered_df['length']=filtered_df['Sequence'].str.len()\n",
        "filtered_df['Sequence'] = filtered_df['Sequence'].str.upper()\n",
        "filtered_df['labels']=1\n",
        "#filtered_df=filtered_df[['Sequence','labels']]\n",
        "filtered_df=filtered_df.rename(columns={'Sequence':'seq'})\n",
        "filtered_df['Disease']='HCC'\n",
        "iso_to_exclude = ['non-tumor tissue from patient 11','non-tumor tissue from patient 9','non-tumor tissue from patient 10','tumor tissue from patient 11']\n",
        "\n",
        "# Drop rows containing the specified organisms\n",
        "filtered_df = filtered_df[~filtered_df['Isolation Source'].isin(iso_to_exclude)]\n",
        "iso_to_exclude = ['Non-HCC']\n",
        "\n",
        "# Drop rows containing the specified organisms\n",
        "filtered_df=filtered_df[~filtered_df['loc'].str.contains('Non-HCC', na=False)]\n",
        "filtered_df=filtered_df[~filtered_df['Isolation Source'].str.contains('Isolation Source', na=False)]\n",
        "filtered_data = filtered_df[\n",
        "    filtered_df['Isolation Source'].str.contains('HCC|tumor', case=False, na=False) |\n",
        "    filtered_df['loc'].str.contains('HCC|tumor', case=False, na=False)\n",
        "]\n",
        "filtered_data=filtered_data.drop_duplicates(subset='seq')"
      ],
      "metadata": {
        "id": "YhCBSQvuC5_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset[['Sequence',\n",
        "       'Sequence_description',   'Virus_genotype',\n",
        "        'Isolation_source',  'Disease',\n",
        "       'Vrius_Gene']]\n",
        "filtered_df=filtered_df[['seq',\n",
        "       'loc',   'Genotype',\n",
        "        'Isolation Source',  'Disease',\n",
        "       'Gene']]\n",
        "filtered_df.columns=['Sequence',\n",
        "       'Sequence_description',   'Virus_genotype',\n",
        "        'Isolation_source',  'Disease',\n",
        "       'Vrius_Gene']\n",
        "import pandas as pd\n",
        "\n",
        "final=pd.concat([dataset,filtered_df],axis=0,ignore_index=True)"
      ],
      "metadata": {
        "id": "oFYP4-F7kbbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final=final.loc[final['Disease'] == 'HCC'\t]"
      ],
      "metadata": {
        "id": "InhSUKmfuxSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final=final.drop_duplicates(subset='Sequence')"
      ],
      "metadata": {
        "id": "g-PTovaKxnUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mgenomes = []\n",
        "dissimilarities = []\n",
        "uncovered_regions = []\n",
        "index=[]\n",
        "extracted_data=pd.DataFrame(columns=['Sequence',\n",
        "       'Sequence_description',   'Virus_genotype',\n",
        "        'Isolation_source',  'Disease',\n",
        "       'Vrius_Gene'])\n",
        "import tqdm\n",
        "all_rows = []\n",
        "for i in tqdm.tqdm(range(len(final['Sequence'].values))):\n",
        "      try:\n",
        "        mgenome,dissimilarity,uncovered_reg=modified_data(final['Sequence'].values[i])\n",
        "        row_data = final.iloc[i, :].to_dict()\n",
        "        row_data['seq'] = mgenome\n",
        "        row_data['dissimilarity'] = dissimilarity\n",
        "        row_data['uncovered_reg'] = uncovered_reg\n",
        "        all_rows.append(row_data)\n",
        "      except:\n",
        "        print(\"Something Wrong\")\n",
        "\n",
        "extracted_data = pd.concat([extracted_data, pd.DataFrame(all_rows)], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "YijSJVyxswj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "extracted_data=pd.read_csv(\"/content/drive/MyDrive/Upwork_Projects/HBV/Experiment1/Dataset/hbv_aligned_data(HCC_NHCC).csv\")"
      ],
      "metadata": {
        "id": "tpvW5ss8NzXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates_mask=extracted_data[extracted_data['Disease'] == 'HCC'].duplicated(subset=['seq'])\n",
        "test=extracted_data[extracted_data['Disease'] == 'HCC'][duplicates_mask] #['Sequence_description'].value_counts() #['Sequence'] #.apply(len).value_counts() #.drop_duplicates('seq')['Sequence_description'].value_counts()\n",
        "test.to_csv(\"/content/drive/MyDrive/Upwork_Projects/HBV/Experiment1/Dataset/test_dataset.csv\")"
      ],
      "metadata": {
        "id": "aFlhR0w_hiTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data.to_csv(\"/content/drive/MyDrive/Upwork_Projects/HBV/Experiment1/Dataset/dataset.csv\")"
      ],
      "metadata": {
        "id": "XkTpeI91gto3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boolean mask for rows where 'Disease' is 'HCC'\n",
        "nhcc_mask = extracted_data['Disease'] != 'HCC'\n",
        "\n",
        "# Apply drop_duplicates only to the subset of rows with 'HCC'\n",
        "extracted_data2 =   pd.concat([extracted_data[~nhcc_mask], extracted_data[nhcc_mask].drop_duplicates(subset=['seq'], keep='first')], ignore_index=True).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VH56JSAobsHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}